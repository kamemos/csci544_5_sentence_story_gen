{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3fe0ce-8253-4c3a-9dc3-59b7130843e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\nlp-project-8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback,IntervalStrategy\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703d90dc-9840-4441-ac50-c6319e79eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "val_df = pd.read_csv('dataset/val.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "peer_df = pd.read_csv('dataset/human_peer_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8ec264-5c0c-40fd-88e5-0c040e91e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 150\n",
    "max_target = 50\n",
    "batch_size = 3\n",
    "model_checkpoints = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9c0e04-82c0-4772-a258-a1fdfabdb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6d95c0-a455-4dbb-b6fe-c9478adaee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(r):\n",
    "    # print( f\"{r['sentence1']} {r['sentence2']} {r['sentence3']} {r['sentence4']}\")\n",
    "    text = f\"{r['sentence1']} {r['sentence2']} {r['sentence3']} {r['sentence4']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b09b7cf-7d26-46c1-8ef1-c9056d7c9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [process_input(r[1]) for r in train_df.iterrows()]\n",
    "train_y = [r[1]['sentence5'] for r in train_df.iterrows()]\n",
    "\n",
    "val_x = [process_input(r[1]) for r in val_df.iterrows()]\n",
    "val_y = [r[1]['sentence5'] for r in val_df.iterrows()]\n",
    "\n",
    "test_x = [process_input(r[1]) for r in test_df.iterrows()]\n",
    "test_y = [r[1]['sentence5'] for r in test_df.iterrows()]\n",
    "\n",
    "peer_x = [process_input(r[1]) for r in peer_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67f1964-7232-40d3-aef6-94a7b8568018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prep_df(df):\n",
    "#     input_x = []\n",
    "#     input_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140647fc-49f3-48a1-845d-5f834ee7489c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ed3a11-e957-42c7-9fae-ee094dba6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e951d6-3ec6-45e3-9da4-2648f67e1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.texts = encodings\n",
    "        self.targets = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_ids'] = self.texts[idx]['input_ids'] \n",
    "        item['attention_mask'] = self.texts[idx]['attention_mask'] \n",
    "        item['labels'] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281483de-eb71-48e9-a846-1dc98b84863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    model_input = tokenizer(text,  max_length=max_input, padding='max_length', truncation=True)\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9200d0-d46a-4513-bc2c-425cb77c593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_x = [preprocess_data(t) for t in train_x]\n",
    "train_tokenized_y = [preprocess_data(t)['input_ids'] for t in train_y]\n",
    "\n",
    "val_tokenized_x = [preprocess_data(t) for t in val_x]\n",
    "val_tokenized_y = [preprocess_data(t)['input_ids'] for t in val_y]\n",
    "\n",
    "test_tokenized_x = [preprocess_data(t) for t in test_x]\n",
    "test_tokenized_y = [preprocess_data(t)['input_ids'] for t in test_y]\n",
    "\n",
    "peer_tokenized_x = [preprocess_data(t) for t in peer_x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488e738e-b3eb-4fec-9ae0-992077016c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokenized_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4803ab8-d78c-43cb-b708-8928825fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_size = 1000\n",
    "\n",
    "# train_tokenized_x = train_tokenized_x[:sampling_size]\n",
    "# train_tokenized_y = train_tokenized_y[:sampling_size]\n",
    "\n",
    "# val_tokenized_x = val_tokenized_x[:sampling_size]\n",
    "# val_tokenized_y = val_tokenized_y [:sampling_size]\n",
    "\n",
    "# test_tokenized_x = test_tokenized_x[:sampling_size]\n",
    "# test_tokenized_y = test_tokenized_y [:sampling_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c82c8b9-da7a-4946-ba72-c2bdf1b18a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= RocDataset(train_tokenized_x,train_tokenized_y)\n",
    "val_data= RocDataset(val_tokenized_x,val_tokenized_y)\n",
    "test_data= RocDataset(test_tokenized_x,test_tokenized_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ff15bd-6b7a-4477-b391-c0a84d3908c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de67c6-1200-4061-aa65-209d908707f7",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bba0642-8e8e-40a1-813c-d3f5168022e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae60a147-1c25-4e2f-b194-96fc6f329dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4284\\2071624228.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('rouge')\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n",
    "\n",
    "metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e679c18-ad28-46b3-a8e9-2d0d48b6fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collator to create batches. It preprocess data with the given tokenizer\n",
    "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1d3584-a162-46ae-873c-bd901e52fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# metrics\n",
    "# compute rouge for evaluation \n",
    "#####################\n",
    "\n",
    "def compute_rouge(pred):\n",
    "    predictions, labels = pred\n",
    "    #decode the predictions\n",
    "    decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    #decode labels\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    #compute results\n",
    "    res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
    "    #get %\n",
    "    res = {key: value.mid.fmeasure * 100 for key, value in res.items()}\n",
    "\n",
    "    pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    res['gen_len'] = np.mean(pred_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75e0695e-df2c-4c23-a98d-5c66bd6b4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./gpt2-gerchef\", #The output directory\n",
    "#     overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "#     num_train_epochs=3, # number of training epochs\n",
    "#     per_device_train_batch_size=32, # batch size for training\n",
    "#     per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "#     eval_steps = 400, # Number of update steps between two evaluations.\n",
    "#     save_steps=800, # after # steps model is saved \n",
    "#     warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "#     prediction_loss_only=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a625ada-9e0b-42c7-bdee-c82fc7c82f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = transformers.Seq2SeqTrainingArguments(\n",
    "    'story_generation',\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps = 4000,\n",
    "    save_steps= 4000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size= 5,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=30,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True \n",
    "    )\n",
    "#only CUDA available -> fp16=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3757a8-d809-43e3-8c78-9e4ce59ddb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_rouge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a42f2cb-3173-4c0e-87de-f96d61179cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\nlp-project-8\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 120000\n",
      "  Number of trainable parameters = 139420416\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24000' max='120000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24000/120000 2:22:23 < 9:29:36, 2.81 it/s, Epoch 6/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.181272</td>\n",
       "      <td>22.140900</td>\n",
       "      <td>5.598400</td>\n",
       "      <td>20.626700</td>\n",
       "      <td>20.629300</td>\n",
       "      <td>12.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.180543</td>\n",
       "      <td>22.409100</td>\n",
       "      <td>5.998600</td>\n",
       "      <td>20.968800</td>\n",
       "      <td>20.964300</td>\n",
       "      <td>12.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.180078</td>\n",
       "      <td>23.120500</td>\n",
       "      <td>6.145000</td>\n",
       "      <td>21.274800</td>\n",
       "      <td>21.286700</td>\n",
       "      <td>13.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>23.562900</td>\n",
       "      <td>6.406300</td>\n",
       "      <td>21.850800</td>\n",
       "      <td>21.858700</td>\n",
       "      <td>12.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.184119</td>\n",
       "      <td>24.037200</td>\n",
       "      <td>6.707500</td>\n",
       "      <td>22.317900</td>\n",
       "      <td>22.319500</td>\n",
       "      <td>12.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.188432</td>\n",
       "      <td>23.971700</td>\n",
       "      <td>6.627200</td>\n",
       "      <td>22.185300</td>\n",
       "      <td>22.184100</td>\n",
       "      <td>12.456800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to story_generation\\checkpoint-4000\n",
      "Configuration saved in story_generation\\checkpoint-4000\\config.json\n",
      "Model weights saved in story_generation\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in story_generation\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in story_generation\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to story_generation\\checkpoint-8000\n",
      "Configuration saved in story_generation\\checkpoint-8000\\config.json\n",
      "Model weights saved in story_generation\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in story_generation\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in story_generation\\checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [story_generation\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to story_generation\\checkpoint-12000\n",
      "Configuration saved in story_generation\\checkpoint-12000\\config.json\n",
      "Model weights saved in story_generation\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in story_generation\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in story_generation\\checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [story_generation\\checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to story_generation\\checkpoint-16000\n",
      "Configuration saved in story_generation\\checkpoint-16000\\config.json\n",
      "Model weights saved in story_generation\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in story_generation\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in story_generation\\checkpoint-16000\\special_tokens_map.json\n",
      "Deleting older checkpoint [story_generation\\checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to story_generation\\checkpoint-20000\n",
      "Configuration saved in story_generation\\checkpoint-20000\\config.json\n",
      "Model weights saved in story_generation\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in story_generation\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in story_generation\\checkpoint-20000\\special_tokens_map.json\n",
      "Deleting older checkpoint [story_generation\\checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to story_generation\\checkpoint-24000\n",
      "Configuration saved in story_generation\\checkpoint-24000\\config.json\n",
      "Model weights saved in story_generation\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in story_generation\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in story_generation\\checkpoint-24000\\special_tokens_map.json\n",
      "Deleting older checkpoint [story_generation\\checkpoint-20000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from story_generation\\checkpoint-12000 (score: 0.18007788062095642).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24000, training_loss=0.19162510013580322, metrics={'train_runtime': 8545.5261, 'train_samples_per_second': 140.424, 'train_steps_per_second': 14.042, 'total_flos': 2.143604736e+16, 'train_loss': 0.19162510013580322, 'epoch': 6.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa44d3dd-a037-4644-ac02-384a977c1f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\n",
      "Configuration saved in model\\config.json\n",
      "Model weights saved in model\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\tokenizer_config.json\n",
      "Special tokens file saved in model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48129758-f12d-4b39-bb9c-7bc390f06064",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01f0b63b-eedd-4c45-81fe-7bac317c2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(test_sample,  max_length=max_input, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ecd784c-9c44-4081-ba05-f09241b30f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_pred, _, _ = trainer.predict([model_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eca8f75-8f10-40cf-bd6c-e56609531e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The pizza was delivered to Jimmy's house the next day.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(raw_pred, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a89f7414-b671-4331-9296-e53dbfe77d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jimmy was too lazy to cook dinner. He decided to order a mushroom pizza to his home. When his pizza arrived, he got an anchovy pizza instead. Jimmy complained about his incorrect order through the phone.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c872278-a347-4efd-9ebf-a72cd751623a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"</s><s>The pizza was delivered to Jimmy's house the next day.</s><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(raw_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fb7b8da-d179-4037-b7f2-4ccd35cf69dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was able to get a new pizza delivered to his home.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "051595fc-8db7-4e0d-9ce6-830936f5f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.predict(train_tokenized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d1edd0-294b-44e1-9568-28d7aff48401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_prediction = trainer.predict(train_tokenized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a0bf51c-755a-4839-9ab6-ac389e3c642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_prediction = trainer.predict(val_tokenized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57c5a672-8cdc-4787-96c3-8468e99ca86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction = trainer.predict(test_tokenized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "137dd4c8-2e9e-41dd-9180-8bb3b2c22b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 50\n",
      "  Batch size = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peer_prediction = trainer.predict(peer_tokenized_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bbdb855-a3f3-49ad-b9f4-3804e7248324",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_train_prediction = [tokenizer.decode(p,skip_special_tokens=True) for p in train_prediction[0]]\n",
    "decode_val_prediction = [tokenizer.decode(p,skip_special_tokens=True) for p in val_prediction[0]]\n",
    "decode_test_prediction = [tokenizer.decode(p,skip_special_tokens=True) for p in test_prediction[0]]\n",
    "decode_peer_prediction = [tokenizer.decode(p,skip_special_tokens=True) for p in peer_prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b059f3b0-321f-4e99-8289-fd96de98b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prediction'] = decode_train_prediction \n",
    "val_df['prediction'] = decode_val_prediction \n",
    "test_df['prediction'] = decode_test_prediction \n",
    "peer_df['prediction'] = decode_peer_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ab85aa2-6ff7-492c-b4de-489b402350af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('dataset/train_prediction.csv',index=False)\n",
    "val_df.to_csv('dataset/val_prediction.csv',index=False)\n",
    "test_df.to_csv('dataset/test_prediction.csv',index=False)\n",
    "peer_df.to_csv('dataset/peer_prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514142f0-1bb2-45ad-8594-04e5b49ba458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
