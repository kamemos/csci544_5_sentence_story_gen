{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WqC2lMSPVlW"
      },
      "outputs": [],
      "source": [
        "#%pip install tensorflow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# set seeds for reproducability\n",
        "import tensorflow\n",
        "from numpy.random import seed\n",
        "tensorflow.random.set_seed(2)\n",
        "seed(1)\n",
        "# keras module for building LSTM \n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "\n",
        "with open('train.csv') as story:\n",
        "  df = pd.read_csv(story)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# reading the data from local google drive \n",
        "with open('/content/rocstories_winter2017.txt') as story:\n",
        "  story_data = story.read()\n",
        "\n",
        "#print(story_data)\n",
        "\n",
        "# data cleaning process\n",
        "import re                              \n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(r',', '', text)\n",
        "  text = re.sub(r'\\'', '',  text)\n",
        "  text = re.sub(r'\\\"', '', text)\n",
        "  text = re.sub(r'\\(', '', text)\n",
        "  text = re.sub(r'\\)', '', text)\n",
        "  text = re.sub(r'\\n', '', text)\n",
        "  text = re.sub(r'“', '', text)\n",
        "  text = re.sub(r'”', '', text)\n",
        "  text = re.sub(r'’', '', text)\n",
        "  text = re.sub(r'\\.', '', text)\n",
        "  text = re.sub(r';', '', text)\n",
        "  text = re.sub(r':', '', text)\n",
        "  text = re.sub(r'\\-', '', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# cleaning the data\n",
        "lower_data = story_data.lower()          \n",
        "\n",
        "split_data = lower_data.splitlines()     \n",
        "\n",
        "#print(split_data)                         \n",
        "\n",
        "final = ''                             \n",
        "\n",
        "for line in split_data:\n",
        "  line = clean_text(line)\n",
        "  final += '\\n' + line\n",
        "\n",
        "\n",
        "final_data = final.split('\\n')     \n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Instantiating the Tokenizer\n",
        "max_vocab = 1000000\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(final_data)\n",
        "\n",
        "# Getting the total number of words of the data.\n",
        "word2idx = tokenizer.word_index\n",
        "vocab_size = len(word2idx) + 1       \n",
        "\n",
        "# We will turn the sentences to sequences line by line and create n_gram sequences\n",
        "\n",
        "input_seq = []\n",
        "\n",
        "for line in final_data:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_seq = token_list[:i+1]\n",
        "    input_seq.append(n_gram_seq)\n",
        "\n",
        "\n",
        "# Getting the maximum length of sequence for padding purpose\n",
        "max_seq_length = max(len(x) for x in input_seq)\n",
        "#print(max_seq_length)\n",
        "\n",
        "# Padding the sequences and converting them to array\n",
        "input_seq = np.array(pad_sequences(input_seq, maxlen=max_seq_length, padding='pre'))\n",
        "#print(input_seq)\n",
        "\n",
        "# Taking xs and labels to train the model.\n",
        "\n",
        "xs = input_seq[:, :-1]        \n",
        "labels = input_seq[:, -1]     \n",
        "#print(\"xs: \",xs)\n",
        "#print(\"labels:\",labels)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# one-hot encoding the labels according to the vocab size\n",
        "\n",
        "\n",
        "ys = to_categorical(labels, num_classes=vocab_size)\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# using the functional APIs of keras to define the model\n",
        "\n",
        "i = Input(shape=(max_seq_length - 1, ))                           # using 1 less value becasuse we are preserving the last value for predicted word \n",
        "x = Embedding(vocab_size, 124)(i)\n",
        "x = Dropout(0.2)(x)\n",
        "x = LSTM(520, return_sequences=True)(x)\n",
        "x = Bidirectional(layer=LSTM(340, return_sequences=True))(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "model = Model(i,x)\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.summary()                                       # We can know about the shape of the model\n",
        "\n",
        "r = model.fit(xs,ys,epochs=100)\n",
        "\n",
        "# Evaluating the model on accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['accuracy'])\n",
        "\n",
        "# Defining a function to take input of seed text from user and no. of words to be predicted\n",
        "\n",
        "def predict_words(seed, no_words):\n",
        "  for i in range(no_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=1)\n",
        "\n",
        "    new_word = ''\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if predicted == index:\n",
        "        new_word = word\n",
        "        break\n",
        "    seed += \" \" + new_word\n",
        "  print(seed)\n",
        "\n",
        "seed_text = 'i am feeling good today'\n",
        "next_words = 10\n",
        "\n",
        "predict_words(seed_text, next_words)\n",
        "\n",
        "# saving the model\n",
        "\n",
        "model.save('LSTM_gen.h5') # Will create a HDF5 file of the model"
      ],
      "metadata": {
        "id": "ROa9a_rOVIGp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}